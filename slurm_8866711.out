The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
The token `llama3` has been saved to /dev/shm/hf-home/stored_tokens
Your token has been saved to /dev/shm/hf-home/token
Login successful.
The current active token is: `llama3`
/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
Unsloth: Failed to patch Gemma3ForConditionalGeneration.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-20 12:10:00 [__init__.py:239] Automatically detected platform cuda.
[WARNING|logging.py:328] 2025-04-20 12:17:07,685 >> Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
Running GRPO script
Sun Apr 20 12:10:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-PCIE-12GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   36C    P0             31W /  250W |     257MiB /  12288MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1638626      C   python                                        254MiB |
+-----------------------------------------------------------------------------------------+

Unsloth: vLLM does not work on older GPUs - will switch to Unsloth inference!
==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.2.
   \\   /|    Tesla P100-PCIE-12GB. Num GPUs = 1. Max memory: 11.901 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
/home/users/ap794/final_project_distillLLM/minillm/results/qwen2.5/train/sft/qwen2.5-1.5B-Instruct/e10-bs1-lr1e-05-G2-N4-NN1/8000 does not have a padding token! Will use pad_token = <|vision_pad|>.
Sun Apr 20 12:17:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-PCIE-12GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   36C    P0             33W /  250W |    1481MiB /  12288MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1638626      C   python                                       1478MiB |
+-----------------------------------------------------------------------------------------+

Sun Apr 20 12:17:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-PCIE-12GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   36C    P0             32W /  250W |    1587MiB /  12288MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1638626      C   python                                       1584MiB |
+-----------------------------------------------------------------------------------------+

Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.
We will change the batch size of 1 to the `num_generations` of 6
Generating train split:   0%|          | 0/47780 [00:00<?, ? examples/s]Generating train split:   2%|â–         | 1000/47780 [00:00<00:37, 1233.08 examples/s]Generating train split:   4%|â–         | 2000/47780 [00:01<00:22, 2020.49 examples/s]Generating train split:   6%|â–‹         | 3000/47780 [00:01<00:19, 2285.01 examples/s]Generating train split:   8%|â–Š         | 4000/47780 [00:01<00:15, 2899.24 examples/s]Generating train split:  12%|â–ˆâ–        | 5778/47780 [00:02<00:14, 2851.25 examples/s]Generating train split:  14%|â–ˆâ–        | 6778/47780 [00:02<00:13, 2975.48 examples/s]Generating train split:  16%|â–ˆâ–‹        | 7778/47780 [00:02<00:12, 3160.73 examples/s]Generating train split:  18%|â–ˆâ–Š        | 8778/47780 [00:03<00:10, 3580.61 examples/s]Generating train split:  22%|â–ˆâ–ˆâ–       | 10556/47780 [00:03<00:11, 3268.94 examples/s]Generating train split:  24%|â–ˆâ–ˆâ–       | 11556/47780 [00:03<00:10, 3478.11 examples/s]Generating train split:  26%|â–ˆâ–ˆâ–‹       | 12556/47780 [00:04<00:10, 3335.62 examples/s]Generating train split:  28%|â–ˆâ–ˆâ–Š       | 13556/47780 [00:04<00:09, 3715.33 examples/s]Generating train split:  32%|â–ˆâ–ˆâ–ˆâ–      | 15334/47780 [00:05<00:09, 3333.65 examples/s]Generating train split:  34%|â–ˆâ–ˆâ–ˆâ–      | 16334/47780 [00:05<00:09, 3337.70 examples/s]Generating train split:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 17334/47780 [00:05<00:09, 3345.21 examples/s]Generating train split:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18334/47780 [00:05<00:07, 4035.62 examples/s]Generating train split:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20112/47780 [00:06<00:08, 3433.14 examples/s]Generating train split:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21112/47780 [00:06<00:07, 3353.89 examples/s]Generating train split:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 22112/47780 [00:07<00:07, 3245.99 examples/s]Generating train split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24890/47780 [00:07<00:06, 3450.53 examples/s]Generating train split:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25890/47780 [00:08<00:06, 3458.06 examples/s]Generating train split:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 26890/47780 [00:08<00:06, 3440.02 examples/s]Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 27890/47780 [00:08<00:05, 3942.85 examples/s]Generating train split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 29668/47780 [00:09<00:05, 3515.76 examples/s]Generating train split:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30668/47780 [00:09<00:04, 3611.89 examples/s]Generating train split:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 31668/47780 [00:09<00:04, 3643.23 examples/s]Generating train split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32668/47780 [00:09<00:03, 4090.16 examples/s]Generating train split:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 34446/47780 [00:10<00:04, 3279.75 examples/s]Generating train split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35446/47780 [00:10<00:03, 3277.28 examples/s]Generating train split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 36446/47780 [00:11<00:03, 3519.08 examples/s]Generating train split:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 37446/47780 [00:11<00:02, 4173.20 examples/s]Generating train split:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39224/47780 [00:11<00:02, 3293.43 examples/s]Generating train split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40224/47780 [00:12<00:02, 3508.06 examples/s]Generating train split:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 41224/47780 [00:12<00:01, 3341.35 examples/s]Generating train split:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42224/47780 [00:12<00:01, 3786.48 examples/s]Generating train split:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44002/47780 [00:13<00:01, 3412.63 examples/s]Generating train split:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45002/47780 [00:13<00:00, 3285.84 examples/s]Generating train split:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46002/47780 [00:13<00:00, 2994.45 examples/s]Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47002/47780 [00:14<00:00, 3639.98 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:14<00:00, 3395.18 examples/s]
Map:   0%|          | 0/47780 [00:00<?, ? examples/s]Map:   2%|â–         | 724/47780 [00:00<00:06, 7143.09 examples/s]Map:   4%|â–Ž         | 1708/47780 [00:00<00:10, 4370.58 examples/s]Map:   5%|â–         | 2322/47780 [00:00<00:13, 3352.21 examples/s]Map:   6%|â–Œ         | 2959/47780 [00:00<00:11, 4026.29 examples/s]Map:   8%|â–Š         | 3608/47780 [00:00<00:13, 3304.42 examples/s]Map:   9%|â–‰         | 4340/47780 [00:01<00:16, 2637.94 examples/s]Map:  10%|â–ˆ         | 5000/47780 [00:01<00:15, 2732.58 examples/s]Map:  12%|â–ˆâ–        | 5673/47780 [00:01<00:12, 3356.77 examples/s]Map:  13%|â–ˆâ–Ž        | 6311/47780 [00:01<00:12, 3198.85 examples/s]Map:  14%|â–ˆâ–        | 6865/47780 [00:02<00:11, 3609.25 examples/s]Map:  16%|â–ˆâ–Œ        | 7624/47780 [00:02<00:13, 2992.03 examples/s]Map:  17%|â–ˆâ–‹        | 8000/47780 [00:02<00:15, 2538.27 examples/s]Map:  18%|â–ˆâ–Š        | 8608/47780 [00:02<00:12, 3101.17 examples/s]Map:  19%|â–ˆâ–‰        | 9313/47780 [00:02<00:12, 3016.45 examples/s]Map:  21%|â–ˆâ–ˆ        | 10000/47780 [00:03<00:13, 2841.30 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 10628/47780 [00:03<00:10, 3389.61 examples/s]Map:  24%|â–ˆâ–ˆâ–Ž       | 11333/47780 [00:03<00:12, 2926.90 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 12000/47780 [00:03<00:12, 2935.15 examples/s]Map:  26%|â–ˆâ–ˆâ–‹       | 12623/47780 [00:03<00:10, 3461.60 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 13320/47780 [00:04<00:10, 3251.21 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 13970/47780 [00:04<00:08, 3814.43 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 14647/47780 [00:04<00:09, 3450.23 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 15314/47780 [00:04<00:10, 3173.30 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 15970/47780 [00:04<00:08, 3747.81 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 16616/47780 [00:05<00:09, 3322.37 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 17352/47780 [00:05<00:10, 2947.60 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18000/47780 [00:05<00:10, 2907.86 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 18675/47780 [00:05<00:08, 3508.71 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 19347/47780 [00:06<00:09, 3133.75 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20000/47780 [00:06<00:09, 3059.37 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 20653/47780 [00:06<00:07, 3628.74 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21331/47780 [00:06<00:07, 3426.96 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 21942/47780 [00:06<00:06, 3910.06 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 22647/47780 [00:06<00:07, 3481.60 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 23319/47780 [00:07<00:07, 3081.16 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24000/47780 [00:07<00:08, 2902.67 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24573/47780 [00:07<00:06, 3343.34 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25000/47780 [00:07<00:08, 2637.35 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 25626/47780 [00:07<00:06, 3228.68 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 26321/47780 [00:08<00:07, 2999.11 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 26898/47780 [00:08<00:06, 3472.12 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 27605/47780 [00:08<00:06, 2897.01 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28000/47780 [00:08<00:07, 2599.16 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 28639/47780 [00:08<00:05, 3221.51 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 29314/47780 [00:09<00:06, 2826.12 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30000/47780 [00:09<00:06, 2742.60 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30681/47780 [00:09<00:05, 3376.73 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 31350/47780 [00:09<00:05, 3211.73 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 31957/47780 [00:09<00:04, 3707.50 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32637/47780 [00:10<00:04, 3354.54 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33310/47780 [00:10<00:04, 2947.39 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 33893/47780 [00:10<00:04, 3410.70 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 34628/47780 [00:10<00:04, 3123.71 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35311/47780 [00:11<00:04, 2802.30 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 35942/47780 [00:11<00:03, 3333.56 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 36654/47780 [00:11<00:03, 2994.14 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 37335/47780 [00:11<00:03, 3022.44 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 37985/47780 [00:11<00:02, 3580.95 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 38602/47780 [00:12<00:02, 3231.82 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39000/47780 [00:12<00:03, 2651.08 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 39680/47780 [00:12<00:02, 3333.76 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40340/47780 [00:12<00:02, 2949.04 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 40981/47780 [00:12<00:01, 3531.22 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 41626/47780 [00:13<00:02, 3028.86 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42296/47780 [00:13<00:02, 2673.08 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 42976/47780 [00:13<00:01, 3294.65 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43687/47780 [00:13<00:01, 2981.44 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44341/47780 [00:14<00:01, 2870.37 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45000/47780 [00:14<00:01, 2736.49 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45617/47780 [00:14<00:00, 3251.10 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46258/47780 [00:14<00:00, 2739.41 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46800/47780 [00:14<00:00, 3150.36 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47371/47780 [00:15<00:00, 2543.69 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:15<00:00, 2555.13 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:15<00:00, 3110.04 examples/s]
[WARNING|<string>:173] 2025-04-20 12:18:06,981 >> ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 47,780 | Num Epochs = 1 | Total steps = 250
O^O/ \_/ \    Batch size per device = 6 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (6 x 1 x 1) = 6
 "-____-"     Trainable parameters = 36,929,536/5,000,000,000 (0.74% trained)
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: alejandro-paredeslatorre to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[{'content': '\n    Respond in the following format:\n    <think>\n    ...\n    </think>\n    <answer>\n    ...\n    </answer>\n    ', 'role': 'system'}, {'content': 'You will be given a competitive programming problem. Please reason step by step about the solution, then provide a complete implementation in C++17.\n\nYour solution must read input from standard input (cin), write output to standard output (cout).\nDo not include any debug prints or additional output.\n\nPut your final solution within a single code block:\n```cpp\n<your code here>\n```\n\n# Problem\n\nYou are given an array $$$a$$$ of $$$n$$$ integers, where $$$n$$$ is odd.\n\nIn one operation, you will remove two adjacent elements from the array $$$a$$$, and then concatenate the remaining parts of the array. For example, given the array $$$[4,7,4,2,9]$$$, we can obtain the arrays $$$[4,2,9]$$$ and $$$[4,7,9]$$$ by the operations $$$[\\underline{4,7}, 4,2,9] \\to [4,2,9]$$$ and $$$[4,7,\\underline{4,2},9] \\to [4,7,9]$$$ respectively. However, we cannot obtain the array $$$[7,2,9]$$$ as it requires deleting non-adjacent elements $$$[\\underline{4},7,\\underline{4},2,9]$$$.\n\nYou will repeatedly perform this operation until exact', 'role': 'user'}]
```cpp
#include <bits/stdc++.h>
using namespace std;

int main() {
    int t;
    cin >> t;
    while (t--) {
        int n;
        cin >> n;
        vector<int> a(n);
        for (int i = 0; i < n; ++i) {
            cin >> a[i];
        }
        int max_val = a[0];
        for (int i = 2; i < n; i += 2) {
            if (a[i] > max_val) {
                max_val = a[i];
            }
        }
        cout << max_val << '\n';
    }
    return 0;
}
```
Sun Apr 20 12:18:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-PCIE-12GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   36C    P0             32W /  250W |    1587MiB /  12288MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1638626      C   python                                       1584MiB |
+-----------------------------------------------------------------------------------------+

wandb: ERROR failed to upsert bucket: returned error 403: {"data":{"upsertBucket":null},"errors":[{"message":"permission denied","path":["upsertBucket"],"extensions":{"code":"PERMISSION_ERROR"}}]}
Traceback (most recent call last):
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/src/main.py", line 236, in <module>
    main()
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/src/main.py", line 188, in main
    trainer.train()
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "<string>", line 223, in _fast_inner_training_loop
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer_callback.py", line 506, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 930, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 857, in setup
    self._wandb.init(
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1544, in init
    wandb._sentry.reraise(e)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 987, in init
    raise error
wandb.errors.errors.CommError: failed to upsert bucket: returned error 403: {"data":{"upsertBucket":null},"errors":[{"message":"permission denied","path":["upsertBucket"],"extensions":{"code":"PERMISSION_ERROR"}}]}
Traceback (most recent call last):
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/src/main.py", line 236, in <module>
    main()
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/src/main.py", line 188, in main
    trainer.train()
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "<string>", line 223, in _fast_inner_training_loop
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer_callback.py", line 506, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 930, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 857, in setup
    self._wandb.init(
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1544, in init
    wandb._sentry.reraise(e)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/home/users/ap794/final_project_distillLLM/aleGRPO/grpo_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 987, in init
    raise error
wandb.errors.errors.CommError: failed to upsert bucket: returned error 403: {"data":{"upsertBucket":null},"errors":[{"message":"permission denied","path":["upsertBucket"],"extensions":{"code":"PERMISSION_ERROR"}}]}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moutputs[0m at: [34mhttps://wandb.ai/alejandro-paredeslatorre/llama-cot-training/runs/hl9v6cyy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250420_121808-hl9v6cyy/logs[0m
srun: error: linux41: task 0: Exited with exit code 1
